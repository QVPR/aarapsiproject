{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/env python3\n",
    "\n",
    "%load_ext autoreload\n",
    "# %autoreload 0\n",
    "%autoreload\n",
    "\n",
    "import copy\n",
    "from itertools import product\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt, ticker\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.collections import LineCollection\n",
    "from matplotlib.patches import FancyArrow\n",
    "from matplotlib.legend_handler import HandlerPatch\n",
    "\n",
    "import scienceplots # for ieee style\n",
    "import seaborn as sns\n",
    "\n",
    "from pyaarapsi.vpr_simple.vpr_helpers import FeatureType\n",
    "from pyaarapsi.pathing.basic import calc_path_stats\n",
    "from pyaarapsi.core.transforms import apply_homogeneous_transform, Transform_Builder\n",
    "\n",
    "from pyaarapsi.nn.enums import ApplyModel\n",
    "from pyaarapsi.nn.vpr_helpers import make_load_vpr_dataset\n",
    "from pyaarapsi.nn.visualize import make_legend_arrow\n",
    "from pyaarapsi.nn.visualize import adj_make_split_axes_y_linlog, mean_confidence_interval\n",
    "from pyaarapsi.nn.params import DFExperiment1, Experiment1, DFExperiment2, Experiment2, DFGeneral, \\\n",
    "    General, DFNNTrain, DFNNTest, NNGeneral\n",
    "\n",
    "from experiment_functions import make_load_experiment_2\n",
    "\n",
    "plt.style.use('ieee')\n",
    "\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=np.VisibleDeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "EXPERIMENT1     = Experiment1()\n",
    "DF_EXPERIMENT1  = DFExperiment1()\n",
    "EXPERIMENT2     = Experiment2()\n",
    "DF_EXPERIMENT2  = DFExperiment2()\n",
    "DF_GENERAL      = DFGeneral()\n",
    "GENERAL         = General()\n",
    "DF_NN_TRAIN     = DFNNTrain()\n",
    "DF_NN_TEST      = DFNNTest()\n",
    "NN_GENERAL      = NNGeneral()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All feature types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "_df_features_fusion = {}\n",
    "_df_features_office = {}\n",
    "_df_features_campus = {}\n",
    "\n",
    "DF_NN_TRAIN_FUSION = DFNNTrain()\n",
    "DF_NN_TRAIN_FUSION.APPLY_MODEL = ApplyModel.USE_FUSED\n",
    "DF_NN_TRAIN_OFFICE = DFNNTrain()\n",
    "DF_NN_TRAIN_OFFICE.APPLY_MODEL = ApplyModel.USE_OFFICE\n",
    "DF_NN_TRAIN_CAMPUS = DFNNTrain()\n",
    "DF_NN_TRAIN_CAMPUS.APPLY_MODEL = ApplyModel.USE_CAMPUS\n",
    "\n",
    "GENERAL.VPR_DP.printer = None if GENERAL.VPRDP_VERBOSE else lambda *args, **kwargs: None\n",
    "GENERAL.VPR_DP.use_tqdm = GENERAL.VPRDP_VERBOSE\n",
    "\n",
    "for FEATURE_TYPE, FEATURE_NAME in zip(DF_GENERAL.FEATURE_TYPES, DF_GENERAL.FEATURE_NAMES):\n",
    "    \n",
    "    data_fusion = make_load_experiment_2(  \n",
    "        feature_type=FEATURE_TYPE, df_nn_train=DF_NN_TRAIN_FUSION,\n",
    "        df_nn_test=DF_NN_TEST, nn_general=NN_GENERAL,\n",
    "        df_exp1=DF_EXPERIMENT1, exp1=EXPERIMENT1, \n",
    "        df_exp2=DF_EXPERIMENT2, exp2=EXPERIMENT2,\n",
    "        df_general=DF_GENERAL, general=GENERAL)\n",
    "\n",
    "    data_fusion['vprfeaturetype'] = FEATURE_TYPE.name\n",
    "    data_fusion['vprfeaturename'] = FEATURE_NAME\n",
    "    data_fusion['svmfactor1type'] = DF_NN_TRAIN_FUSION.VPR.SVM_FACTORS[FEATURE_TYPE.name][0]\n",
    "    data_fusion['svmfactor2type'] = DF_NN_TRAIN_FUSION.VPR.SVM_FACTORS[FEATURE_TYPE.name][1]\n",
    "\n",
    "    _df_features_fusion[FEATURE_TYPE.name] = copy.deepcopy(data_fusion)\n",
    "    del data_fusion\n",
    "\n",
    "    data_office = make_load_experiment_2(\n",
    "        feature_type=FEATURE_TYPE, df_nn_train=DF_NN_TRAIN_OFFICE,\n",
    "        df_nn_test=DF_NN_TEST, nn_general=NN_GENERAL,\n",
    "        df_exp1=DF_EXPERIMENT1, exp1=EXPERIMENT1, \n",
    "        df_exp2=DF_EXPERIMENT2, exp2=EXPERIMENT2,\n",
    "        df_general=DF_GENERAL, general=GENERAL)\n",
    "\n",
    "    data_office['vprfeaturetype'] = FEATURE_TYPE.name\n",
    "    data_office['vprfeaturename'] = FEATURE_NAME\n",
    "    data_office['svmfactor1type'] = DF_NN_TRAIN_OFFICE.VPR.SVM_FACTORS[FEATURE_TYPE.name][0]\n",
    "    data_office['svmfactor2type'] = DF_NN_TRAIN_OFFICE.VPR.SVM_FACTORS[FEATURE_TYPE.name][1]\n",
    "\n",
    "    _df_features_office[FEATURE_TYPE.name] = copy.deepcopy(data_office)\n",
    "    del data_office\n",
    "\n",
    "    data_campus = make_load_experiment_2(\n",
    "        feature_type=FEATURE_TYPE, df_nn_train=DF_NN_TRAIN_CAMPUS,\n",
    "        df_nn_test=DF_NN_TEST, nn_general=NN_GENERAL,\n",
    "        df_exp1=DF_EXPERIMENT1, exp1=EXPERIMENT1, \n",
    "        df_exp2=DF_EXPERIMENT2, exp2=EXPERIMENT2,\n",
    "        df_general=DF_GENERAL, general=GENERAL)\n",
    "\n",
    "    data_campus['vprfeaturetype'] = FEATURE_TYPE.name\n",
    "    data_campus['vprfeaturename'] = FEATURE_NAME\n",
    "    data_campus['svmfactor1type'] = DF_NN_TRAIN_CAMPUS.VPR.SVM_FACTORS[FEATURE_TYPE.name][0]\n",
    "    data_campus['svmfactor2type'] = DF_NN_TRAIN_CAMPUS.VPR.SVM_FACTORS[FEATURE_TYPE.name][1]\n",
    "\n",
    "    _df_features_campus[FEATURE_TYPE.name] = copy.deepcopy(data_campus)\n",
    "    del data_campus\n",
    "\n",
    "del FEATURE_TYPE, FEATURE_NAME\n",
    "\n",
    "df_features_fusion = pd.concat([_df_features_fusion[i] for i in _df_features_fusion.keys()])\n",
    "df_features_office = pd.concat([_df_features_office[i] for i in _df_features_office.keys()])\n",
    "df_features_campus = pd.concat([_df_features_campus[i] for i in _df_features_campus.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # For some reason, this plot breaks in Seaborn v0.13.0\n",
    "    # pip install seaborn==0.12.2; pgrep python | xargs kill -9\n",
    "    # then rerun script.\n",
    "\n",
    "    _s = 4.5/4.0\n",
    "\n",
    "    fig_m1, axes_m1         = plt.subplots(3,1,figsize=(4*_s,_s*1.1*3), sharex=True)\n",
    "\n",
    "    axes_m1_plotter         = lambda ft_type, ax: sns.boxplot(\n",
    "        data=df_features_fusion[(df_features_fusion['vprfeaturetype']==ft_type)], \n",
    "        y='pos_error',x='set',hue='MODE',ax=ax, palette=General.PALETTE, whis=1000,\n",
    "        hue_order=General.HUE_ORDER, order=General.SET_ORDER, linewidth=0.5, width=0.8, \n",
    "        flierprops=dict(alpha=.05, marker='.', markersize=4, markeredgecolor='none', markerfacecolor='k'),\n",
    "        capprops=dict(color='k'),\n",
    "        boxprops=dict(edgecolor='k'), whiskerprops=dict(color='k'), medianprops=dict(color='k'))\n",
    "    \n",
    "    axes_m1log0 = adj_make_split_axes_y_linlog(axes_m1[0], [-0.1, 1, 205], \n",
    "                    lambda ax: axes_m1_plotter('APGEM', ax), size=0.3, swap_order=False)\n",
    "    axes_m1log1 = adj_make_split_axes_y_linlog(axes_m1[1], [-0.1, 1, 205], \n",
    "                    lambda ax: axes_m1_plotter('NETVLAD', ax), size=0.3, swap_order=False)\n",
    "    axes_m1log2 = adj_make_split_axes_y_linlog(axes_m1[2], [-0.1, 1, 20.5], \n",
    "                    lambda ax: axes_m1_plotter('SALAD', ax), size=0.3, swap_order=False)\n",
    "\n",
    "    _formatter              = ticker.FormatStrFormatter(\"%0.1f\")\n",
    "    axes_m1[0].yaxis.set_major_formatter(_formatter)\n",
    "    axes_m1[1].yaxis.set_major_formatter(_formatter)\n",
    "    axes_m1[2].yaxis.set_major_formatter(_formatter)\n",
    "    axes_m1log0.yaxis.set_major_formatter(_formatter)\n",
    "    axes_m1log1.yaxis.set_major_formatter(_formatter)\n",
    "    axes_m1log2.yaxis.set_major_formatter(_formatter)\n",
    "\n",
    "    axes_m1[0].tick_params(bottom=False)\n",
    "    axes_m1[1].tick_params(bottom=False)\n",
    "\n",
    "    small_frame0 = fig_m1.add_subplot(311, frameon=False)\n",
    "    small_frame1 = fig_m1.add_subplot(312, frameon=False)\n",
    "    small_frame2 = fig_m1.add_subplot(313, frameon=False)\n",
    "\n",
    "    small_frame0.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "    small_frame1.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "    small_frame2.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "\n",
    "    small_frame0.set_ylabel('APGeM',      fontsize=9, labelpad=5)\n",
    "    small_frame1.set_ylabel('NetVLAD',  fontsize=9, labelpad=5)\n",
    "    small_frame2.set_ylabel('SALAD',    fontsize=9, labelpad=5)\n",
    "\n",
    "    big_frame = fig_m1.add_subplot(111, frameon=False)\n",
    "    big_frame.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False)\n",
    "    big_frame.set_ylabel('Localization Error (m)',  labelpad=22, fontsize=9)\n",
    "    big_frame.set_title('Experiment 2: Along-Track Localization Error Distribution', fontsize=9)\n",
    "\n",
    "    lgnd = axes_m1[2].legend(loc='lower center', frameon=False, ncol=5, bbox_to_anchor=(0.48,-1.2),\n",
    "                             fontsize=9, labelspacing=0, columnspacing=1.5, handletextpad=0.6)\n",
    "\n",
    "    axes_m1[0].tick_params(axis='both', which='both', labelsize=9)\n",
    "    axes_m1[1].tick_params(axis='both', which='both', labelsize=9)\n",
    "    axes_m1[2].tick_params(axis='both', which='both', labelsize=9)\n",
    "    axes_m1log0.tick_params(axis='both', which='both', labelsize=9)\n",
    "    axes_m1log1.tick_params(axis='both', which='both', labelsize=9)\n",
    "    axes_m1log2.tick_params(axis='both', which='both', labelsize=9)\n",
    "\n",
    "    fig_m1.subplots_adjust(hspace=0.1)\n",
    "\n",
    "    fig_m1.canvas.draw()\n",
    "\n",
    "    path = GENERAL.DIR_MEDIA + '/exp2_new_extended_key_results_vertical' #relative to file directory\n",
    "    fig_m1.savefig(path+'.pdf', format='pdf', bbox_extra_artists=(lgnd,), dpi=300, pad_inches=0, bbox_inches='tight')\n",
    "    fig_m1.savefig(path+'.png', format='png', bbox_extra_artists=(lgnd,), dpi=300, pad_inches=0, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    LATEX_MODE = True\n",
    "    ONLY_AGGREGATES = True\n",
    "    TWO_TABLES = True\n",
    "    UNDERLINE_MODE = False # latex only\n",
    "\n",
    "    offset = '        '\n",
    "    if LATEX_MODE:\n",
    "        _la  = '% 13s'\n",
    "        _sb  = r'\\B{'\n",
    "        _si  = r'\\I{'\n",
    "        _su  = r'\\U{'\n",
    "        _sn  = r'   '\n",
    "        _eb  = r'm  }'\n",
    "        _ei  = r'm  }'\n",
    "        _en  = r'm   '\n",
    "        _ebp = r'\\% }'\n",
    "        _eip = r'\\% }'\n",
    "        _enp = r'\\%  '\n",
    "        _ee  = r' \\\\'\n",
    "        _sf  = '& % 14s '\n",
    "        if UNDERLINE_MODE:\n",
    "            _si = _su\n",
    "    else:\n",
    "        _la  = '% 11s'\n",
    "        _sb  = r''\n",
    "        _si  = r''\n",
    "        _sn  = r''\n",
    "        _eb  = r'm* '\n",
    "        _ei  = r'm^ '\n",
    "        _en  = r'm  '\n",
    "        _ebp = r'%%*'\n",
    "        _eip = r'%%^'\n",
    "        _enp = r'%% '\n",
    "        _ee  = r'   '\n",
    "        _sf  = '| % 8s '\n",
    "    print()\n",
    "    long_featname = np.max([len(i) for i in DF_GENERAL.FEATURE_NAMES])\n",
    "    df_ = df_features_fusion[(df_features_fusion['slice_length']*10).astype(int).isin([15])]\n",
    "\n",
    "    def make_xfix(_b, _i, _n, c, bests, best_inds, skip_inds):\n",
    "        '''\n",
    "        _b -> prefix if bold\n",
    "        _i -> prefix if italics\n",
    "        _n -> prefix if normal\n",
    "        c  -> index of best_inds in question\n",
    "        '''\n",
    "        italics_inds = []\n",
    "        italics_values = []\n",
    "        bold_inds = []\n",
    "        bold_values = []\n",
    "        bold_first = -1\n",
    "        for p, i in enumerate(best_inds):\n",
    "            if (i not in skip_inds):\n",
    "                if (not bold_inds) or (bests[i] in bold_values):\n",
    "                    bold_inds.append(i)\n",
    "                    bold_values.append(bests[i])\n",
    "                    if bold_first == -1:\n",
    "                        bold_first = p\n",
    "        for p, i in enumerate(best_inds):\n",
    "            if (i in skip_inds):\n",
    "                # if an italics_ind value beats the best bold_value\n",
    "                if ((not italics_inds) and ((p < bold_first) or (bests[i] in bold_values))) \\\n",
    "                    or (bests[i] in italics_values):\n",
    "                    italics_inds.append(i)\n",
    "                    italics_values.append(bests[i])\n",
    "        return _b if c in bold_inds else (_i if c in italics_inds else (_n))\n",
    "\n",
    "    def table_start():\n",
    "        print(r'    \\centering')\n",
    "        print(r'    \\scriptsize')\n",
    "        print(r'    \\setlength\\tabcolsep{1.2mm}')\n",
    "        print(r'    \\begin{tabular}{lcL{11mm}|M{9mm}|M{9mm}M{9mm}|M{9mm}M{9mm}|}')\n",
    "\n",
    "    def table_end(table_label: str):\n",
    "        print(r'    \\end{tabular}')\n",
    "        print(r'    \\label{table:' + table_label + r'}')\n",
    "        print(r'    \\vspace*{-\\baselineskip}')\n",
    "\n",
    "    def make_main_table(show_a_t_stats: bool, show_perc_stats: bool):\n",
    "        thresh_inds = [GENERAL.PRED_TYPES.index('nvp'), GENERAL.PRED_TYPES.index('nvr')]\n",
    "        num_rows = (3 if show_a_t_stats else 0) + (2 if show_perc_stats else 0)\n",
    "        for featname, feat in zip(DF_GENERAL.FEATURE_NAMES, \n",
    "                                  [i.name for i in DF_GENERAL.FEATURE_TYPES]):\n",
    "            if LATEX_MODE:\n",
    "                if feat != DF_GENERAL.FEATURE_TYPES[0].name:\n",
    "                    if not ONLY_AGGREGATES:\n",
    "                        print(offset + r'\\hline')\n",
    "                else:\n",
    "                    if not ONLY_AGGREGATES:\n",
    "                        print(offset + r'\\cline{3-7}')\n",
    "                    else:\n",
    "                        print(offset + r'\\cline{4-8}')\n",
    "                if not ONLY_AGGREGATES:\n",
    "                    print(offset + r'& & \\multicolumn{5}{c|}{\\B{' + featname + r' Features}} \\\\')\n",
    "                if feat == DF_GENERAL.FEATURE_TYPES[0].name:\n",
    "                    sep = r' & ' if not ONLY_AGGREGATES else r' & & '\n",
    "                    print(offset + r'&' + sep \n",
    "                        + r'\\B{Baseline} & \\multicolumn{4}{c|}{\\B{Filtering Technique}} \\\\')\n",
    "                    print(offset + r'&' + sep \n",
    "                        + r'\\B{VPR}      & \\boldmath{$N_P$} & '\n",
    "                          r'\\boldmath{$N_R$} & \\B{SVM} & \\B{Ours} \\\\')\n",
    "                print(offset + r'\\hline')\n",
    "            else:\n",
    "                print('\\n' + featname)\n",
    "            for env, cond in product(['Office', 'Campus', 'Aggregate'], ['Normal', 'Adverse']):\n",
    "                if (env == 'Aggregate'): \n",
    "                    if (cond == 'Adverse'): break\n",
    "                    else: set_name = env\n",
    "                else:\n",
    "                    if ONLY_AGGREGATES: continue\n",
    "                    set_name = env + '\\n' + cond\n",
    "                \n",
    "                if not (set_name == 'Aggregate'):\n",
    "                    df__ = df_[(df_['set'] == set_name) & (df_['vprfeaturetype']==feat)]\n",
    "                else:\n",
    "                    df__ = df_[(df_['vprfeaturetype']==feat)]\n",
    "\n",
    "                stats = {}\n",
    "                for label in GENERAL.PRED_TYPES:\n",
    "                    stats[label] = {k: np.nan \n",
    "                                    for k in ['mean', 'conf', 'med', 'max', 'prec', 'rcll']}\n",
    "                    data = df__[df__['mode']==label]\n",
    "                    if (data['pos_error'] != np.nan).sum() > 0:\n",
    "                        stats[label]['mean'], stats[label]['conf'] = mean_confidence_interval(\n",
    "                            data['pos_error'].to_numpy(dtype=float))\n",
    "                        stats[label]['med'] = np.nanmedian(data['pos_error'].to_numpy(dtype=float))\n",
    "                        stats[label]['max'] = np.nanmax(data['pos_error'].to_numpy(dtype=float))\n",
    "                        stats[label]['prec'] = 100 * (data['TP'].sum() \\\n",
    "                                                / (data['TP'].sum() + data['FP'].sum()))\n",
    "                        stats[label]['rcll'] = 100 * (data['TP'].sum() \\\n",
    "                                                / (data['TP'].sum() + data['FN'].sum()))\n",
    "\n",
    "                num_labels = len(stats.keys())\n",
    "                means = np.round(np.array([stats[label]['mean'] for label in stats.keys()]),2)\n",
    "                means_best = np.argsort(means)\n",
    "                means_str = ''.join([_sf % (make_xfix(_sb, _si, _sn, i, means, means_best, thresh_inds) \n",
    "                                    + ('%6.2f' % means[i])\n",
    "                                    + make_xfix(_eb, _ei, _en, i, means, means_best, thresh_inds))\n",
    "                                            for i in range(num_labels)])\n",
    "\n",
    "                medis = np.round(np.array([stats[label]['med'] for label in stats.keys()]),2)\n",
    "                medis_best = np.argsort(medis)\n",
    "                medis_str = ''.join([_sf % (make_xfix(_sb, _si, _sn, i, medis, medis_best, thresh_inds) \n",
    "                                    + ('%6.2f' % medis[i])\n",
    "                                    + make_xfix(_eb, _ei, _en, i, medis, medis_best, thresh_inds))\n",
    "                                            for i in range(num_labels)])\n",
    "\n",
    "                max_s = np.round(np.array([stats[label]['max'] for label in stats.keys()]),2)\n",
    "                max_s_best = np.argsort(max_s)\n",
    "                max_s_str = ''.join([_sf % (make_xfix(_sb, _si, _sn, i, max_s, max_s_best, thresh_inds) \n",
    "                                    + ('%6.2f' % max_s[i])\n",
    "                                    + make_xfix(_eb, _ei, _en, i, max_s, max_s_best, thresh_inds))\n",
    "                                            for i in range(num_labels)])\n",
    "                \n",
    "                precs = np.round(np.array([stats[label]['prec'] for label in stats.keys()]),2)\n",
    "                precs_best = np.argsort(precs)[::-1]\n",
    "                precs_str = ''.join([_sf % (make_xfix(_sb, _si, _sn, i, precs, precs_best, thresh_inds) \n",
    "                                    + ('%6.2f' % precs[i])\n",
    "                                    + make_xfix(_ebp, _eip, _enp, i, precs, precs_best, thresh_inds))\n",
    "                                            for i in range(num_labels)])\n",
    "                \n",
    "                rclls = np.round(np.array([stats[label]['rcll'] for label in stats.keys()]),2)\n",
    "                rclls_best = np.argsort(rclls)[::-1]\n",
    "                rclls_str = ''.join([_sf % (make_xfix(_sb, _si, _sn, i, rclls, rclls_best, thresh_inds) \n",
    "                                    + ('%6.2f' % rclls[i])\n",
    "                                    + make_xfix(_ebp, _eip, _enp, i, rclls, rclls_best, thresh_inds))\n",
    "                                            for i in range(num_labels)])\n",
    "\n",
    "                _set_name = set_name.replace('\\n',' ')\n",
    "                if not LATEX_MODE:\n",
    "                    print(_set_name)\n",
    "                    print((' '*7) + ''.join([_la % i \n",
    "                                                for i in ['VPR', 'NVP', 'NVR', 'SVM', 'NN']]))\n",
    "                if LATEX_MODE:\n",
    "                    if not ONLY_AGGREGATES:\n",
    "                        _first_whole = r'\\ocMCSMR{1}{|c|}{' + str(num_rows)  \\\n",
    "                                + r'}{' + _set_name + r'}' \\\n",
    "                                + (' '*5 if env == 'Aggregate' else \\\n",
    "                                        (' ' if cond == 'Normal' else ''))\n",
    "                        _other = r'\\ocblb'\n",
    "                        _other_whole = _other + (' ' * (len(_first_whole) - len(_other)))\n",
    "                    else:\n",
    "                        featname_diff = long_featname - len(featname)\n",
    "                        _first_whole = r'\\ocMCSMRT{2}{|c|}{' + str(num_rows)  \\\n",
    "                                + r'}{' + featname + r' Aggregate}' \\\n",
    "                            + (' ' * featname_diff)\n",
    "                        _other = r'\\ocblbT'\n",
    "                        _other_whole = _other + (' ' * (len(_first_whole) - len(_other)))\n",
    "                    if show_a_t_stats:\n",
    "                        print(offset + _first_whole + ' &      Mean ' + means_str + _ee)\n",
    "                        print(offset + _other_whole + ' &    Median ' + medis_str + _ee)\n",
    "                        print(offset + _other_whole + ' &   Maximum ' + max_s_str + _ee)\n",
    "                    if show_perc_stats:\n",
    "                        if show_a_t_stats:\n",
    "                            print(offset + _other_whole + ' & Precision ' + precs_str + _ee)\n",
    "                        else:\n",
    "                            print(offset + _first_whole + ' & Precision ' + precs_str + _ee)\n",
    "                        print(offset + _other_whole + ' &    Recall ' + rclls_str + _ee)\n",
    "                    if env != 'Aggregate':\n",
    "                        print(offset + '\\hline')\n",
    "                else:\n",
    "                    if show_a_t_stats:\n",
    "                        print('     Mean: ' + means_str)\n",
    "                        print('   Median: ' + medis_str)\n",
    "                        print('  Maximum: ' + max_s_str)\n",
    "                    if show_perc_stats:\n",
    "                        print('Precision: ' + precs_str)\n",
    "                        print('   Recall: ' + rclls_str)\n",
    "        if LATEX_MODE:\n",
    "            print(offset + '\\hline')\n",
    "    \n",
    "    if not TWO_TABLES:\n",
    "        if LATEX_MODE:\n",
    "            print(r'\\begin{table}[!ht]')\n",
    "            print(r'    \\caption{Experiment 2 along-track localization error in meters, as well as '\n",
    "                r'system precision and recall. Best '\n",
    "                  r'performance for each feature type is indicated by bold, ignoring naive '\n",
    "                  r'thresholds. Naive thresholds are italicized if best performance.}')\n",
    "            table_start()\n",
    "        make_main_table(True, True)\n",
    "        if LATEX_MODE:\n",
    "            table_end(\"exp2table_new\")\n",
    "            print(r'\\end{table}')\n",
    "    else:\n",
    "        if LATEX_MODE:\n",
    "            print(r'\\begin{table}[!ht]')\n",
    "            print(r'    \\caption{Experiment 2 along-track localization error in meters. Best '\n",
    "                  r'performance for each feature type is indicated by bold, ignoring naive '\n",
    "                  r'thresholds. Naive thresholds are italicized if best performance.}')\n",
    "            table_start()\n",
    "        make_main_table(True, False)\n",
    "        if LATEX_MODE:\n",
    "            table_end(\"exp2table_new\")\n",
    "            print(r'\\end{table}')\n",
    "        print()\n",
    "        if LATEX_MODE:\n",
    "            print(r'\\begin{table}[!ht]')\n",
    "            print(r'    \\caption{Experiment 2 system precision and recall. Best '\n",
    "                  r'performance for each feature type is indicated by bold, ignoring naive '\n",
    "                  r'thresholds. Naive thresholds are italicized if best performance.}')\n",
    "            table_start()\n",
    "        make_main_table(False, True)\n",
    "        if LATEX_MODE:\n",
    "            table_end(\"exp2table2_new\")\n",
    "            print(r'\\end{table}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features_fusion.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    LATEX_MODE = False\n",
    "    ONLY_AGGREGATES = False\n",
    "    TWO_TABLES = True\n",
    "    UNDERLINE_MODE = False # latex only\n",
    "\n",
    "    offset = '        '\n",
    "    if LATEX_MODE:\n",
    "        _la  = '% 14s'\n",
    "        _sb  = r'\\B{'\n",
    "        _si  = r'\\I{'\n",
    "        _su  = r'\\U{'\n",
    "        _sn  = r'   '\n",
    "        _eb  = r'm  }'\n",
    "        _ei  = r'm  }'\n",
    "        _en  = r'm   '\n",
    "        _ebp = r'\\% }'\n",
    "        _eip = r'\\% }'\n",
    "        _enp = r'\\%  '\n",
    "        _ee  = r' \\\\'\n",
    "        _sf  = '& % 14s '\n",
    "        if UNDERLINE_MODE:\n",
    "            _si = _su\n",
    "    else:\n",
    "        _la  = '% 12s'\n",
    "        _sb  = r''\n",
    "        _si  = r''\n",
    "        _sn  = r''\n",
    "        _eb  = r'm* '\n",
    "        _ei  = r'm^ '\n",
    "        _en  = r'm  '\n",
    "        _ebp = r'%%*'\n",
    "        _eip = r'%%^'\n",
    "        _enp = r'%% '\n",
    "        _ee  = r'   '\n",
    "        _sf  = '| % 8s '\n",
    "    print()\n",
    "\n",
    "    def make_xfix(_b, _i, _n, c, bests, best_inds, skip_inds):\n",
    "        '''\n",
    "        _b -> prefix if bold\n",
    "        _i -> prefix if italics\n",
    "        _n -> prefix if normal\n",
    "        c  -> index of best_inds in question\n",
    "        '''\n",
    "        italics_inds = []\n",
    "        italics_values = []\n",
    "        bold_inds = []\n",
    "        bold_values = []\n",
    "        bold_first = -1\n",
    "        for p, i in enumerate(best_inds):\n",
    "            if (i not in skip_inds):\n",
    "                if (not bold_inds) or (bests[i] in bold_values):\n",
    "                    bold_inds.append(i)\n",
    "                    bold_values.append(bests[i])\n",
    "                    if bold_first == -1:\n",
    "                        bold_first = p\n",
    "        for p, i in enumerate(best_inds):\n",
    "            if (i in skip_inds):\n",
    "                # if an italics_ind value beats the best bold_value\n",
    "                if ((not italics_inds) and ((p < bold_first) or (bests[i] in bold_values))) \\\n",
    "                    or (bests[i] in italics_values):\n",
    "                    italics_inds.append(i)\n",
    "                    italics_values.append(bests[i])\n",
    "        return _b if c in bold_inds else (_i if c in italics_inds else (_n))\n",
    "\n",
    "    def table_start():\n",
    "        print(r'    \\centering')\n",
    "        print(r'    \\scriptsize')\n",
    "        print(r'    \\setlength\\tabcolsep{1.2mm}')\n",
    "        print(r'    \\begin{tabular}{lcL{11mm}|M{9mm}|M{9mm}M{9mm}|M{9mm}M{9mm}|}')\n",
    "\n",
    "    def table_end(table_label: str):\n",
    "        print(r'    \\end{tabular}')\n",
    "        print(r'    \\label{table:' + table_label + r'}')\n",
    "        print(r'    \\vspace*{-\\baselineskip}')\n",
    "\n",
    "    if LATEX_MODE:\n",
    "        print(r'\\begin{table}[!ht]')\n",
    "        print(r'    \\caption{\\textcolor{red}{TODO Best '\n",
    "                r'performance for each feature type is indicated by bold, ignoring naive '\n",
    "                r'thresholds. Naive thresholds are italicized if best performance.}}')\n",
    "        table_start()\n",
    "\n",
    "    thresh_inds = [GENERAL.PRED_TYPES.index('nvp'), GENERAL.PRED_TYPES.index('nvr')]\n",
    "\n",
    "    for env in ['Office', 'Campus']:\n",
    "        stats = {}\n",
    "        for stat_label, label in [('vpr','vpr'), ('nnf','prd'), ('nno','prd'), ('nnc','prd')]:\n",
    "            stats[stat_label] = {k: np.nan \n",
    "                            for k in ['mean', 'prec', 'rcll']}\n",
    "            if stat_label in ['vpr', 'nnf']:\n",
    "                df_  = df_features_fusion[(df_features_fusion['slice_length']*10).astype(int).isin([15])]\n",
    "                if env == 'any':\n",
    "                    data = df_[(df_['mode']==label)]\n",
    "                else:\n",
    "                    data = df_[(df_['environment'] == env) & (df_['mode']==label)]\n",
    "            elif stat_label == 'nno':\n",
    "                df_  = df_features_office[(df_features_office['slice_length']*10).astype(int).isin([15])]\n",
    "                if env == 'any':\n",
    "                    data = df_[(df_['mode']==label)]\n",
    "                else:\n",
    "                    data = df_[(df_['environment'] == env) & (df_['mode']==label)]\n",
    "            elif stat_label == 'nnc':\n",
    "                df_  = df_features_campus[(df_features_campus['slice_length']*10).astype(int).isin([15])]\n",
    "                if env == 'any':\n",
    "                    data = df_[(df_['mode']==label)]\n",
    "                else:\n",
    "                    data = df_[(df_['environment'] == env) & (df_['mode']==label)]\n",
    "            else:\n",
    "                raise Exception('bad stat_label: ' + stat_label)\n",
    "            \n",
    "            if (data['pos_error'] != np.nan).sum() > 0:\n",
    "                stats[stat_label]['mean'] = mean_confidence_interval( \\\n",
    "                                        data['pos_error'].to_numpy(dtype=float))[0]\n",
    "                stats[stat_label]['med'] = np.nanmedian(data['pos_error'].to_numpy(dtype=float))\n",
    "                stats[stat_label]['max'] = np.nanmax(data['pos_error'].to_numpy(dtype=float))\n",
    "                _precs = []\n",
    "                _rclls = []\n",
    "                for vpr_technique in data['vprfeaturetype'].unique().tolist():\n",
    "                    _data = data[data['vprfeaturetype'] == vpr_technique]\n",
    "                    _precs.append(100 * (_data['TP'].sum() \\\n",
    "                                            / (_data['TP'].sum() + _data['FP'].sum())))\n",
    "                    _rclls.append(100 * (_data['TP'].sum() \\\n",
    "                                            / (_data['TP'].sum() + _data['FN'].sum())))\n",
    "                stats[stat_label]['prec'] = np.mean(_precs)\n",
    "                stats[stat_label]['rcll'] = np.mean(_rclls)\n",
    "                \n",
    "        num_labels = len(stats.keys())\n",
    "        means = np.round(np.array([stats[label]['mean'] for label in stats.keys()]),2)\n",
    "        means_best = np.argsort(means)\n",
    "        means_str = ''.join([_sf % (make_xfix(_sb, _si, _sn, i, means, means_best, thresh_inds) \n",
    "                            + ('%6.2f' % means[i])\n",
    "                            + make_xfix(_eb, _ei, _en, i, means, means_best, thresh_inds))\n",
    "                                    for i in range(num_labels)])\n",
    "\n",
    "        medis = np.round(np.array([stats[label]['med'] for label in stats.keys()]),2)\n",
    "        medis_best = np.argsort(medis)\n",
    "        medis_str = ''.join([_sf % (make_xfix(_sb, _si, _sn, i, medis, medis_best, thresh_inds) \n",
    "                            + ('%6.2f' % medis[i])\n",
    "                            + make_xfix(_eb, _ei, _en, i, medis, medis_best, thresh_inds))\n",
    "                                    for i in range(num_labels)])\n",
    "\n",
    "        max_s = np.round(np.array([stats[label]['max'] for label in stats.keys()]),2)\n",
    "        max_s_best = np.argsort(max_s)\n",
    "        max_s_str = ''.join([_sf % (make_xfix(_sb, _si, _sn, i, max_s, max_s_best, thresh_inds) \n",
    "                            + ('%6.2f' % max_s[i])\n",
    "                            + make_xfix(_eb, _ei, _en, i, max_s, max_s_best, thresh_inds))\n",
    "                                    for i in range(num_labels)])\n",
    "        \n",
    "        precs = np.round(np.array([stats[label]['prec'] for label in stats.keys()]),2)\n",
    "        precs_best = np.argsort(precs)[::-1]\n",
    "        precs_str = ''.join([_sf % (make_xfix(_sb, _si, _sn, i, precs, precs_best, thresh_inds) \n",
    "                            + ('%6.2f' % precs[i])\n",
    "                            + make_xfix(_ebp, _eip, _enp, i, precs, precs_best, thresh_inds))\n",
    "                                    for i in range(num_labels)])\n",
    "        \n",
    "        rclls = np.round(np.array([stats[label]['rcll'] for label in stats.keys()]),2)\n",
    "        rclls_best = np.argsort(rclls)[::-1]\n",
    "        rclls_str = ''.join([_sf % (make_xfix(_sb, _si, _sn, i, rclls, rclls_best, thresh_inds) \n",
    "                            + ('%6.2f' % rclls[i])\n",
    "                            + make_xfix(_ebp, _eip, _enp, i, rclls, rclls_best, thresh_inds))\n",
    "                                    for i in range(num_labels)])\n",
    "    \n",
    "        if not LATEX_MODE:\n",
    "            print(env)\n",
    "            print((' '*11) + '|' + ''.join([_la % i \n",
    "                        for i in ['VPR    |', 'NN Fused |', 'NN Office |', 'NN Campus  ']]))\n",
    "        if LATEX_MODE:\n",
    "            _first_whole = r'\\ocMCSMR{1}{|c|}{3}{' + env + r'}'\n",
    "            _other = r'\\ocblb'\n",
    "            _other_whole = _other + (' ' * (len(_first_whole) - len(_other)))\n",
    "\n",
    "            print(offset + _first_whole + ' &      Mean ' + means_str + _ee)\n",
    "            print(offset + _first_whole + ' &    Median ' + medis_str + _ee)\n",
    "            print(offset + _first_whole + ' &   Maximum ' + max_s_str + _ee)\n",
    "            print(offset + _other_whole + ' & Precision ' + precs_str + _ee)\n",
    "            print(offset + _other_whole + ' &    Recall ' + rclls_str + _ee)\n",
    "        else:\n",
    "            print('     Mean: ' + means_str)\n",
    "            print('   Median: ' + medis_str)\n",
    "            print('  Maximum: ' + max_s_str)\n",
    "            print('Precision: ' + precs_str)\n",
    "            print('   Recall: ' + rclls_str)\n",
    "        if LATEX_MODE:\n",
    "            print(offset + '\\hline')\n",
    "    \n",
    "    if LATEX_MODE:\n",
    "        table_end(\"agg_table\")\n",
    "        print(r'\\end{table}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modes of Improvement Example Figure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "feature_type = FeatureType.APGEM\n",
    "feature_name = \"AP-GeM\"\n",
    "VERBOSE = False\n",
    "if True:\n",
    "    if True: # Dataset Loading\n",
    "        df = df_features_fusion[(df_features_fusion['vprfeaturetype']==feature_type.name)]\n",
    "\n",
    "        qry_subset = DF_GENERAL.TEST_QRY_SUBSETS[feature_type.name]\n",
    "        ref_subset = DF_GENERAL.TEST_REF_SUBSETS[feature_type.name]\n",
    "\n",
    "        dataset_params = {\"combos\": DF_GENERAL.VPR.COMBOS, \"vpr_dp\": GENERAL.VPR_DP,\n",
    "                        \"try_gen\": True, \"verbose\": VERBOSE}\n",
    "\n",
    "        ln_qry = make_load_vpr_dataset(env=\"Office\", cond=\"Normal\",  set_type=\"qry\", subset=qry_subset, **dataset_params)\n",
    "        cn_qry = make_load_vpr_dataset(env=\"Campus\", cond=\"Normal\",  set_type=\"qry\", subset=qry_subset, **dataset_params)\n",
    "        la_qry = make_load_vpr_dataset(env=\"Office\", cond=\"Adverse\", set_type=\"qry\", subset=qry_subset, **dataset_params)\n",
    "        ca_qry = make_load_vpr_dataset(env=\"Campus\", cond=\"Adverse\", set_type=\"qry\", subset=qry_subset, **dataset_params)\n",
    "\n",
    "        ln_ref = make_load_vpr_dataset(env=\"Office\", cond=\"Normal\",  set_type=\"ref\", subset=ref_subset, **dataset_params)\n",
    "        cn_ref = make_load_vpr_dataset(env=\"Campus\", cond=\"Normal\",  set_type=\"ref\", subset=ref_subset, **dataset_params)\n",
    "        la_ref = make_load_vpr_dataset(env=\"Office\", cond=\"Adverse\", set_type=\"ref\", subset=ref_subset, **dataset_params)\n",
    "        ca_ref = make_load_vpr_dataset(env=\"Campus\", cond=\"Adverse\", set_type=\"ref\", subset=ref_subset, **dataset_params)\n",
    "\n",
    "        _qry_hash = {'Office\\nNormal': ln_qry, 'Office\\nAdverse': la_qry, 'Campus\\nNormal': cn_qry, 'Campus\\nAdverse': ca_qry}\n",
    "        _ref_hash = {'Office\\nNormal': ln_ref, 'Office\\nAdverse': la_ref, 'Campus\\nNormal': cn_ref, 'Campus\\nAdverse': ca_ref}\n",
    "\n",
    "    if True: # Make main figure\n",
    "        _env            = 'Office'\n",
    "        _cond           = 'Adverse'\n",
    "        _min            = 923\n",
    "        _num            = 55\n",
    "        _ang            = 0\n",
    "        _all_offset     = [-11.4, -9.35]\n",
    "        _true_offset    = [0.0, 0.15]\n",
    "        _xlim           = [0, 2.001]\n",
    "        _ylim           = [0, 0.301]\n",
    "        _xstep          = 0.5\n",
    "        _ystep          = 0.1\n",
    "        _green_line     = \"#228a3e\"\n",
    "        _red_line       = \"#a12328\"\n",
    "        _true_marker    = \"#3c3cb0\"\n",
    "        _match_marker   = \"#edd81c\"\n",
    "        cols            = tuple(_green_line if i == 'g' else (_red_line if i == 'r' else i)\n",
    "                                for i in \"kgkkrrrrrrrrggggggggggkkkkkk\")\n",
    "\n",
    "        if True:\n",
    "            if True:\n",
    "                _set            = _env+'\\n'+_cond\n",
    "                _qry            = _qry_hash[_set]\n",
    "                _ref            = _ref_hash[_set]\n",
    "                _s = 5.0/4.0\n",
    "                _shared_style   = dict(marker='.', linestyle='none', alpha=1.0, markersize=10, markeredgecolor='k', markeredgewidth=0.2)\n",
    "                lines_style     = dict(linewidths=0.6)\n",
    "\n",
    "                fig_5, axes_5   = plt.subplots(1,2,figsize=(4*_s,1*_s))\n",
    "\n",
    "            if True: # plotting:\n",
    "                vpr_data = df[(df['mode']=='vpr') & (df['set']==_set)]\n",
    "                prd_data = df[(df['mode']=='prd') & (df['set']==_set)]\n",
    "\n",
    "                _qry_wo_odom    = np.stack([_qry['px'][:,1], _qry['py'][:,1], _qry['pw'][:,1]], axis=1)\n",
    "                _ref_gt_odom    = np.stack([_ref['px'],      _ref['py'],      _ref['pw']],      axis=1)\n",
    "\n",
    "                qry_wo_sum, qry_wo_len  = calc_path_stats(_qry_wo_odom[:,0:2])\n",
    "                ref_gt_sum              = calc_path_stats(_ref_gt_odom)[0]\n",
    "                dd_ind_min              = np.argmin(abs(np.array(qry_wo_sum) - 1.5)) + 1\n",
    "                dd_ind_max              = len(_qry_wo_odom)\n",
    "\n",
    "                _max    = np.min([_min + _num, dd_ind_max-dd_ind_min])\n",
    "                _step   = 2\n",
    "                _slice  = slice(_min,_max, _step)\n",
    "                dd      = slice(dd_ind_min, dd_ind_max+1)\n",
    "\n",
    "                match_vpr   = np.array([[_ref['p'+j][i] for j in 'xy'] \n",
    "                                        for i in vpr_data['hist_match_index']])[_slice]\n",
    "                match_prd   = np.array([[_ref['p'+j][i] for j in 'xy'] if not np.isnan(i) else [np.nan]*2\n",
    "                                        for i in prd_data['hist_match_index']])[_slice]\n",
    "                true_vpr    = np.array([[_ref['p'+j][i] for j in 'xy'] \n",
    "                                        for i in vpr_data['hist_truth_index']])[_slice]\n",
    "                true_prd    = np.array([[_ref['p'+j][i] for j in 'xy'] if not np.isnan(i) else [np.nan]*2\n",
    "                                        for i in prd_data['hist_truth_index']])[_slice]\n",
    "                assert len(true_vpr) == len(match_vpr)\n",
    "                assert len(true_prd) == len(match_prd)\n",
    "                tb = Transform_Builder()\n",
    "                _H = tb.rotate(0,0,_ang,radians=False).translate(0.0,0,0).get()\n",
    "\n",
    "                match_vpr = np.stack(apply_homogeneous_transform(_H, match_vpr[:,0], match_vpr[:,1])[0:2], axis=1) + _all_offset\n",
    "                match_prd = np.stack(apply_homogeneous_transform(_H, match_prd[:,0], match_prd[:,1])[0:2], axis=1) + _all_offset\n",
    "                true_vpr  = np.stack(apply_homogeneous_transform(_H, true_vpr[:,0], true_vpr[:,1])[0:2], axis=1) + _true_offset + _all_offset\n",
    "                true_prd  = np.stack(apply_homogeneous_transform(_H, true_prd[:,0], true_prd[:,1])[0:2], axis=1) + _true_offset + _all_offset\n",
    "\n",
    "                lines_vpr = [[(true_vpr[i,0], true_vpr[i,1]), (match_vpr[i,0], match_vpr[i,1])] for i in range(len(true_vpr))]\n",
    "                lines_prd = [[(true_prd[i,0], true_prd[i,1]), (match_prd[i,0], match_prd[i,1])] for i in range(len(true_prd))]\n",
    "\n",
    "                \n",
    "\n",
    "                axes_5[0].add_collection(LineCollection(lines_vpr, **lines_style, colors=cols))\n",
    "                axes_5[0].plot(true_vpr[:,0],   true_vpr[:,1],   markerfacecolor=_true_marker, **_shared_style)\n",
    "                axes_5[0].plot(match_vpr[:,0],  match_vpr[:,1],  markerfacecolor=_match_marker, **_shared_style)\n",
    "                axes_5[0].set_title(\"Baseline VPR\")\n",
    "\n",
    "                axes_5[1].add_collection(LineCollection(lines_prd, **lines_style, colors=cols))\n",
    "                axes_5[1].plot(true_vpr[:,0],   true_vpr[:,1],   markerfacecolor=_true_marker, **_shared_style)\n",
    "                axes_5[1].plot(true_prd[:,0],   true_prd[:,1],   markerfacecolor=_true_marker, **_shared_style, label='True Position')\n",
    "                axes_5[1].plot(match_prd[:,0],  match_prd[:,1],  markerfacecolor=_match_marker, **_shared_style, label='Match Position')\n",
    "                axes_5[1].set_title(GENERAL.MODE_NAMES[\"prd\"] + \" Correction\")\n",
    "\n",
    "            if True: # general:\n",
    "                for ax in np.array(axes_5).flatten().tolist():\n",
    "                    ax.tick_params(axis='both', which='both', direction='in', labelleft=False, \n",
    "                                                    labelbottom=False, right=True, top=True, length=2, width=0.66, \n",
    "                                                    grid_linewidth=0.2, grid_color='k', grid_alpha=0.8)\n",
    "                    \n",
    "                fig_5.subplots_adjust(hspace=0, wspace=0)\n",
    "\n",
    "                axes_5i = fig_5.add_subplot(111, frameon=False)\n",
    "                axes_5i.tick_params(axis='both', which='both', labelleft=False, labelbottom=False, left=False, bottom=False)\n",
    "                axes_5i.set_xlabel(r'Grid: {x: 50cm, y: 1cm}', fontsize=8)\n",
    "\n",
    "            if True: # legend\n",
    "                legend_elements = [ \n",
    "                                    Line2D([0], [0], marker='.', label='           True Position \\n(Where the robot $actually$ is)',  color='k',  linewidth=0, \n",
    "                                            markersize=10, markerfacecolor=_true_marker, markeredgewidth=0.3),\n",
    "                                    Line2D([0], [0], marker='.', label='          Match Position \\n(Where the robot $thinks$ it is)', color='k',  linewidth=0, \n",
    "                                            markersize=10, markerfacecolor=_match_marker, markeredgewidth=0.3),\n",
    "                                ]\n",
    "                lgnd = axes_5[0].legend(handles=legend_elements, loc='lower center', ncol=2, columnspacing=0.4, \n",
    "                                            handletextpad=-0.3, frameon=False, bbox_to_anchor=(0.95,-0.575), \n",
    "                                            handler_map={FancyArrow : HandlerPatch(patch_func=make_legend_arrow)})\n",
    "\n",
    "            if True: # draw background images\n",
    "                axes_5[0].imshow(GENERAL.BGIMG2[:,:], extent=[_xlim[0], _xlim[1], _ylim[0], _ylim[1]], alpha=0.5)\n",
    "                axes_5[0].set_aspect('auto')\n",
    "                axes_5[1].imshow(GENERAL.BGIMG2[:,:], extent=[_xlim[0], _xlim[1], _ylim[0], _ylim[1]], alpha=0.5)\n",
    "                axes_5[1].set_aspect('auto')\n",
    "\n",
    "            if True: # ticks, lims\n",
    "                for ax in np.array(axes_5).flatten().tolist():\n",
    "                    ax.set_xticks(np.arange(_xlim[0],_xlim[1],_xstep))\n",
    "                    ax.set_yticks(np.arange(_ylim[0],_ylim[1],_ystep))\n",
    "                    ax.set_xlim(_xlim[0],_xlim[1])\n",
    "                    ax.set_ylim(_ylim[0],_ylim[1])\n",
    "                    ax.grid(\"on\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    path = GENERAL.DIR_MEDIA + '/exp2_new_along_path_example' #relative to file directory\n",
    "    fig_5.savefig(path+'.pdf', format='pdf', bbox_extra_artists=(lgnd,), dpi=300, pad_inches=0, bbox_inches='tight')\n",
    "    fig_5.savefig(path+'.png', format='png', bbox_extra_artists=(lgnd,), dpi=300, pad_inches=0, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate-Aggregate Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate-aggregate data:\n",
    "data = df_features_fusion[df_features_fusion['mode']=='vpr']\n",
    "vpr_aggregate_mean = np.nanmean(data['pos_error'].to_numpy(dtype=float))\n",
    "vpr_aggregate_med = np.nanmedian(data['pos_error'].to_numpy(dtype=float))\n",
    "vpr_aggregate_max = np.nanmax(data['pos_error'].to_numpy(dtype=float))\n",
    "vpr_aggregate_prec = 100 * data['TP'].sum() / (data['TP'].sum() + data['FP'].sum())\n",
    "vpr_aggregate_rcll = 100 * data['TP'].sum() / (data['TP'].sum() + data['FN'].sum())\n",
    "\n",
    "data = df_features_fusion[df_features_fusion['mode']=='svm']\n",
    "svm_aggregate_mean = np.nanmean(data['pos_error'].to_numpy(dtype=float))\n",
    "svm_aggregate_med = np.nanmedian(data['pos_error'].to_numpy(dtype=float))\n",
    "svm_aggregate_max = np.nanmax(data['pos_error'].to_numpy(dtype=float))\n",
    "svm_aggregate_prec = 100 * data['TP'].sum() / (data['TP'].sum() + data['FP'].sum())\n",
    "svm_aggregate_rcll = 100 * data['TP'].sum() / (data['TP'].sum() + data['FN'].sum())\n",
    "\n",
    "data = df_features_fusion[df_features_fusion['mode']=='prd']\n",
    "nn_aggregate_mean = np.nanmean(data['pos_error'].to_numpy(dtype=float))\n",
    "nn_aggregate_med = np.nanmedian(data['pos_error'].to_numpy(dtype=float))\n",
    "nn_aggregate_max = np.nanmax(data['pos_error'].to_numpy(dtype=float))\n",
    "nn_aggregate_prec = 100 * data['TP'].sum() / (data['TP'].sum() + data['FP'].sum())\n",
    "nn_aggregate_rcll = 100 * data['TP'].sum() / (data['TP'].sum() + data['FN'].sum())\n",
    "\n",
    "print()\n",
    "print(f'      Experiment 2 Aggregate Statistics Improvement from Baseline VPR')\n",
    "print(f'           ----------------------------------------------------')\n",
    "print(f'                          VPR   :   SVM   :    NN')\n",
    "print(f'           ----------------------------------------------------')\n",
    "print(f'                  mean: {vpr_aggregate_mean:6.2f}m : {svm_aggregate_mean:6.2f}m : {nn_aggregate_mean:6.2f}m')\n",
    "print(f'                median: { vpr_aggregate_med:6.2f}m : { svm_aggregate_med:6.2f}m : { nn_aggregate_med:6.2f}m')\n",
    "print(f'               maximum: { vpr_aggregate_max:6.2f}m : { svm_aggregate_max:6.2f}m : { nn_aggregate_max:6.2f}m')\n",
    "print(f'             precision: {vpr_aggregate_prec:6.2f}% : {svm_aggregate_prec:6.2f}% : {nn_aggregate_prec:6.2f}%')\n",
    "print(f'                recall: {vpr_aggregate_rcll:6.2f}% : {svm_aggregate_rcll:6.2f}% : {nn_aggregate_rcll:6.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
